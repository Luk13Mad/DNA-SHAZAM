import pandas as pd
import numpy as np
import argparse
import os
import sys
import itertools as itt

parser = argparse.ArgumentParser(description='feature extraction')
parser.add_argument('-in',"--input", type=str,help='input query file, full path',required=True)
parser.add_argument('-db',"--database", type=str,help='database to search against',required=True)
parser.add_argument('-out',"--output", type=str,help='output directory',default=os.getcwd())

args = vars(parser.parse_args())

'''
script searches query against provided database
'''

def import_query(query):
    '''
    reads .csv file with query
    .csv-file generated by generate_adresses.py
    labels columns 
    
    Parameters
    ----------
    query: input file with data

    Returns
    -------
    dataframe
    '''
    try:
        raw_query=pd.read_csv(query, sep ="\t", header = None, compression='gzip')
    except FileNotFoundError:
        sys.exit("!!!!ERROR!!!!\n>>>query input file not found<<<")
    
    anc=[]
    point=[]
    delta=[]
    tmp2=[]
    for line1,line2 in zip(raw_query[0],raw_query[1]):
        anc.append(int(line1.strip("()").split(",")[0]))
        point.append(int(line1.strip("()").split(",")[1]))
        delta.append(int(line1.strip("()").split(",")[2]))
        tmp2.append(int(line2))
        
    q_table=pd.DataFrame(zip(anc,point,delta,tmp2),columns=["anchor","point","delta","abs_t_query"])
    return q_table

def find_matching_adresses(query,database):
    '''
    find matching adresses and return boolean mask
    
    Parameters
    ----------
    query: input file with data
    database: chunk of database to bbe queried against

    Returns
    -------
    boolean mask
    '''
    idf=pd.DataFrame({"anchor":database["anchor"].isin(query["anchor"]),"point":database["point"].isin(query["point"]),"delta":database["delta"].isin(query["delta"])})
    matching_adresses=idf.all(axis="columns")
    return matching_adresses

def count_couples(query,database,matching_adresses):
    '''
    Count different occurences of couples

    Parameters
    ----------
    query : list with query 
    database: db to search against
    matching_adresses: boolean mask with matching adresses

    Returns
    -------
    pd.Series 
    '''
    
    if True in matching_adresses.value_counts().to_dict().keys():
        print("{}-matched from-{}".format(matching_adresses.value_counts().to_dict()[True],len(query)),file=sys.stderr)
    else:
        print("no matches",file=sys.stderr)
        return
    
    x=database.loc[matching_adresses]
    counts =x["ID"].value_counts()
    if len(counts)==1:
        candidates=counts.keys()
    elif (len(counts)>1) & (len(counts)<=10):
        candidates=counts.keys()[0:3]
    else:
        candidates=counts.keys()[0:6]
    del counts
    return candidates

def count_target_zones(candidates,database,matching_adresses):
    '''
    Count complete target zones

    Parameters
    ----------
    candidates: pd.Series with candidates
    database: db to search against
    matching_adresses: boolean mask with matching adresses

    Returns
    -------
    pd.DataFrame 
    '''
    x=database.loc[matching_adresses]
    reduced_db=x[x["ID"].isin(candidates)]
    tmp=reduced_db.groupby("anchor").filter(lambda x: len(x) == 5)
    return tmp["anchor"]

def extract_time(candidates,database,new_zones,matching_adresses):
    '''
    returns final selection of dataframe

    Parameters
    ----------
    candidates: pd.Series with candidates
    database: db to search against
    new_zones: list with complete target zones
    matching_adresses: boolean mask with matching adresses

    Returns
    -------
    pd.DataFrame 
    '''
    x=database.loc[matching_adresses]
    reduced_db=x[x["anchor"].isin(new_zones)]
    return reduced_db
    
def main(query,database):
    matching_adresses_indx=find_matching_adresses(query,database)
    Candidates=count_couples(query,database,matching_adresses_indx)
    if Candidates is None:
        return
    new_target_zones=count_target_zones(Candidates,database,matching_adresses_indx)
    part_df=extract_time(Candidates,database,new_target_zones,matching_adresses_indx)
    return part_df
    
Query=import_query(args["input"])
Result=pd.DataFrame()

try:
    raw_db=pd.read_csv(args["database"], sep ="\t", header = None, compression='gzip',chunksize=1000)
except FileNotFoundError:
    sys.exit("!!!!ERROR!!!!\n>>>database input file not found<<<")
    
for i,chunk in enumerate(raw_db):
    anc=[]
    point=[]
    delta=[]
    abs_t=[]
    ID=[]
    for line1,line2 in zip(chunk[0],chunk[1]):
        anc.append(int(line1.strip("()").split(",")[0]))
        point.append(int(line1.strip("()").split(",")[1]))
        delta.append(int(line1.strip("()").split(",")[2]))
        abs_t.append(int(line2.strip("()").split(",")[0]))
        ID.append(line2.strip("()").split(",")[1])
    Database=pd.DataFrame(zip(anc,point,delta,abs_t,ID),columns=["anchor","point","delta","abs_t","ID"])
    tmp=main(Query,Database)
    if tmp is not None:
        Result=Result.append(tmp)
x=Result.iloc[:,3:5].groupby("ID",)
t_dict={}
t_dict.update({"query":list(Query["abs_t_query"])})
for name, group in x:
    t_dict.update({name:list(group["abs_t"])})
printdf=pd.DataFrame.from_dict(t_dict, orient='index').transpose()
printdf.to_csv("{2}/time_series_{0}_{1}.csv".format(args["input"].split("/")[-1].split(".")[0],args["database"].split("/")[-1].split(".")[0],args["output"]),sep="\t",header=True,index=False)